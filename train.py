# train.py
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
import torch
import re
import os

# ==================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ====================
MODEL_NAME = "ai-forever/rugpt3small_based_on_gpt2"
CSV_FILE_PATH = "data.csv"           # –£–±–µ–¥–∏—Å—å, —á—Ç–æ —Ñ–∞–π–ª –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ
OUTPUT_DIR = "./my_rugpt3_finetuned"
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs("./logs", exist_ok=True)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# ==================== –û–ß–ò–°–¢–ö–ê –¢–ï–ö–°–¢–ê ====================
def clean_text(text):
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–∞"""
    if pd.isna(text):
        return ""
    text = str(text)
    text = re.sub(r'<[^>]+>', '', text)           # HTML-—Ç–µ–≥–∏
    text = re.sub(r'\s+', ' ', text)              # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\?+', '?', text)              # ?? -> ?
    text = re.sub(r'\!+', '!', text)              # !! -> !
    text = re.sub(r':\s*$', '', text)             # –£–±—Ä–∞—Ç—å –¥–≤–æ–µ—Ç–æ—á–∏–µ –≤ –∫–æ–Ω—Ü–µ
    text = re.sub(r'^[–ê-–Ø][–∞-—è]+\s+[–ê-–Ø]\.[–ê-–Ø]\.:', '–û–ø–µ—Ä–∞—Ç–æ—Ä:', text)  # –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    return text.strip()

def contains_russian(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –∫–∏—Ä–∏–ª–ª–∏—Ü—É"""
    return bool(re.search(r'[–∞-—è–ê-–Ø]', text))

# ==================== –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ====================
def load_and_format_data(csv_path):
    print("üîç –ß—Ç–µ–Ω–∏–µ CSV —Ñ–∞–π–ª–∞...")
    try:
        df = pd.read_csv(
            csv_path,
            header=None,
            names=['question', 'answer'],
            quoting=1,               # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–≤—ã—á–µ–∫
            escapechar='\\',
            on_bad_lines='warn',     # –ü–æ–∫–∞–∂–µ—Ç –æ—à–∏–±–∫–∏, –Ω–æ –Ω–µ —Å–ª–æ–º–∞–µ—Ç—Å—è
            encoding='utf-8'
        )
        print(f"‚úÖ –ü—Ä–æ—á–∏—Ç–∞–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ CSV: {e}")
        return None

    dialog_examples = []
    skipped_no_text = 0
    skipped_english = 0

    for _, row in df.iterrows():
        q = clean_text(row['question'])
        a = clean_text(row['answer'])

        # –ü—Ä–æ–ø—É—Å–∫ –ø—É—Å—Ç—ã—Ö
        if not q or not a:
            skipped_no_text += 1
            continue

        # –§–∏–ª—å—Ç—Ä: —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
        if not contains_russian(q):
            skipped_english += 1
            continue

        # –§–æ—Ä–º–∞—Ç –¥–∏–∞–ª–æ–≥–∞
        dialog = f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {q}\n–°–∏—Å—Ç–µ–º–∞: {a}"
        dialog_examples.append(dialog)

    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(dialog_examples)} –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    print(f"‚ÑπÔ∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –ø—É—Å—Ç—ã—Ö: {skipped_no_text}, –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö: {skipped_english}")

    dataset = Dataset.from_dict({"text": dialog_examples})
    return dataset.train_test_split(test_size=0.1, seed=42)

# ==================== –û–°–ù–û–í–ù–û–ô –ü–†–û–¶–ï–°–° ====================
def main():
    print("=" * 60)
    print("üöÄ –ó–ê–ü–£–©–ï–ù –ü–†–û–¶–ï–°–° –î–û–û–ë–£–ß–ï–ù–ò–Ø RuGPT3")
    print("–ú–æ–¥–µ–ª—å: ai-forever/rugpt3small_based_on_gpt2")
    print("–§–æ—Ä–º–∞—Ç: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å ‚Üí –°–∏—Å—Ç–µ–º–∞")
    print("=" * 60)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    dataset = load_and_format_data(CSV_FILE_PATH)
    if dataset is None:
        return

    print(f"üìä –û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(dataset['train'])}")
    print(f"üìä –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö: {len(dataset['test'])}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    print("\n‚è¨ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        tokenizer.pad_token = tokenizer.eos_token  # –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è GPT-2

        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
        print("‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=256,
            padding="max_length",
            return_tensors=None,
        )

    print("üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
    tokenized_datasets = dataset.map(
        tokenize_function,
        batched=True,
        num_proc=4,
        remove_columns=["text"]
    )

    # –ö–æ–ª–ª–∞—Ç–æ—Ä
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8
    )

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        num_train_epochs=3,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=2,
        learning_rate=3e-5,
        warmup_steps=200,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=10,
        save_steps=500,
        eval_steps=500,
        eval_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to="none",
        fp16=torch.cuda.is_available(),  # –ê–≤—Ç–æ-–≤–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è GPU
        dataloader_num_workers=4,
        dataloader_pin_memory=True,
        remove_unused_columns=True,
        disable_tqdm=False,
        seed=42,
    )

    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        data_collator=data_collator,
        tokenizer=tokenizer,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
    )

    # –û–±—É—á–µ–Ω–∏–µ
    print("\nüî• –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï...")
    try:
        trainer.train()
        print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        trainer.save_model()
        tokenizer.save_pretrained(OUTPUT_DIR)
        print(f"üéâ –£–°–ü–ï–®–ù–û: –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{OUTPUT_DIR}'")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()