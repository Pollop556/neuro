# train.py
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
import torch
import re
import os
import numpy as np
from sklearn.metrics import accuracy_score
import logging

# ==================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ====================
MODEL_NAME = "ai-forever/rugpt3small_based_on_gpt2"
CSV_FILE_PATH = "data_shuffled.csv"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ—à–∞–Ω–Ω—ã–π —Ñ–∞–π–ª!
OUTPUT_DIR = "./my_rugpt3_finetuned"
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs("./logs", exist_ok=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# ==================== –û–ß–ò–°–¢–ö–ê –¢–ï–ö–°–¢–ê ====================
def clean_text(text):
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–∞"""
    if pd.isna(text):
        return ""
    text = str(text)
    text = re.sub(r'<[^>]+>', '', text)           # HTML-—Ç–µ–≥–∏
    text = re.sub(r'http\S+', '', text)           # –°—Å—ã–ª–∫–∏
    text = re.sub(r'[‚Äú‚Äù¬´¬ª]', '"', text)           # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–≤—ã—á–µ–∫
    text = re.sub(r'[^\w\s\.\,\!\?\:\;\-\+\"\'\(\)\‚Äî]', ' ', text)  # –¢–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è
    text = re.sub(r'\s+', ' ', text)              # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\?+', '?', text)              # ?? -> ?
    text = re.sub(r'\!+', '!', text)              # !! -> !
    text = re.sub(r':\s*$', '', text)             # –£–±—Ä–∞—Ç—å –¥–≤–æ–µ—Ç–æ—á–∏–µ –≤ –∫–æ–Ω—Ü–µ
    text = re.sub(r'^[–ê-–Ø][–∞-—è]+\s+[–ê-–Ø]\.[–ê-–Ø]\.:', '–û–ø–µ—Ä–∞—Ç–æ—Ä:', text)  # –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    return text.strip()

def contains_russian(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –∫–∏—Ä–∏–ª–ª–∏—Ü—É"""
    return bool(re.search(r'[–∞-—è–ê-–Ø]', text))

def calculate_average_length(dataset):
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ max_length"""
    lengths = []
    for example in dataset:
        text = example['text'] if 'text' in example else example
        lengths.append(len(text.split()))
    return np.mean(lengths)

# ==================== –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ====================
def load_and_format_data(csv_path):
    print("üîç –ß—Ç–µ–Ω–∏–µ CSV —Ñ–∞–π–ª–∞...")
    try:
        df = pd.read_csv(
            csv_path,
            header=None,
            names=['question', 'answer'],
            quoting=1,
            escapechar='\\',
            on_bad_lines='warn',
            encoding='utf-8'
        )
        print(f"‚úÖ –ü—Ä–æ—á–∏—Ç–∞–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫
        print("\nüìã –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
        for i in range(min(3, len(df))):
            print(f"  {i+1}. –í: {df.iloc[i]['question'][:60]}...")
            print(f"     –û: {df.iloc[i]['answer'][:60]}...")
            print()
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ CSV: {e}")
        return None

    dialog_examples = []
    skipped_no_text = 0
    skipped_english = 0
    length_stats = []

    for _, row in df.iterrows():
        q = clean_text(row['question'])
        a = clean_text(row['answer'])

        # –ü—Ä–æ–ø—É—Å–∫ –ø—É—Å—Ç—ã—Ö
        if not q or not a:
            skipped_no_text += 1
            continue

        # –§–∏–ª—å—Ç—Ä: —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
        if not contains_russian(q):
            skipped_english += 1
            continue

        # –§–æ—Ä–º–∞—Ç –¥–∏–∞–ª–æ–≥–∞
        dialog = f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {q}\n–°–∏—Å—Ç–µ–º–∞: {a}"
        dialog_examples.append(dialog)
        length_stats.append(len(dialog.split()))

    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(dialog_examples)} –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    print(f"‚ÑπÔ∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –ø—É—Å—Ç—ã—Ö: {skipped_no_text}, –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö: {skipped_english}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–æ–≤
    if length_stats:
        avg_length = np.mean(length_stats)
        max_length = np.max(length_stats)
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω—ã: —Å—Ä–µ–¥–Ω—è—è = {avg_length:.1f}, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è = {max_length}")
        print(f"üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π max_length: {int(max_length * 1.2)}")

    dataset = Dataset.from_dict({"text": dialog_examples})
    return dataset.train_test_split(test_size=0.15, seed=42, shuffle=True)  # –£–≤–µ–ª–∏—á–∏–ª–∏ test_size

# ==================== –ú–ï–¢–†–ò–ö–ò ====================
def compute_metrics(eval_pred):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=2)
    
    # –í—ã—á–∏—Å–ª—è–µ–º accuracy —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ-pad —Ç–æ–∫–µ–Ω–æ–≤
    mask = labels != -100
    correct = (predictions == labels) & mask
    accuracy = correct.sum() / mask.sum()
    
    return {"accuracy": accuracy}

# ==================== –û–°–ù–û–í–ù–û–ô –ü–†–û–¶–ï–°–° ====================
def main():
    print("=" * 60)
    print("üöÄ –ó–ê–ü–£–©–ï–ù –ü–†–û–¶–ï–°–° –î–û–û–ë–£–ß–ï–ù–ò–Ø RuGPT3")
    print(f"–ú–æ–¥–µ–ª—å: {MODEL_NAME}")
    print(f"–î–∞–Ω–Ω—ã–µ: {CSV_FILE_PATH}")
    print("–§–æ—Ä–º–∞—Ç: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å ‚Üí –°–∏—Å—Ç–µ–º–∞")
    print("=" * 60)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    dataset = load_and_format_data(CSV_FILE_PATH)
    if dataset is None:
        return

    print(f"üìä –û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(dataset['train'])}")
    print(f"üìä –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö: {len(dataset['test'])}")

    # –ê–Ω–∞–ª–∏–∑ —Å—Ä–µ–¥–Ω–µ–π –¥–ª–∏–Ω—ã –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ max_length
    avg_length = calculate_average_length(dataset['train'])
    max_length = min(512, int(avg_length * 1.5))  # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π max_length
    print(f"üî§ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω max_length: {max_length}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    print("\n‚è¨ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        
        # –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ pad_token –¥–ª—è GPT
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )
        
        # –ù–µ –Ω—É–∂–Ω–æ resize_token_embeddings –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ eos_token –∫–∞–∫ pad_token
        print("‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        print(f"üí° –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(tokenizer)}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º max_length
    def tokenize_function(examples):
        tokenized = tokenizer(
            examples["text"],
            truncation=True,
            max_length=max_length,
            padding="max_length",
            return_tensors=None,
        )
        return tokenized

    print("üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
    tokenized_datasets = dataset.map(
        tokenize_function,
        batched=True,
        num_proc=4,
        remove_columns=["text"]
    )

    # –ö–æ–ª–ª–∞—Ç–æ—Ä
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8
    )

    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –¥–ª—è 769 –ø—Ä–∏–º–µ—Ä–æ–≤
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        num_train_epochs=3,                    # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        per_device_train_batch_size=4,         # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        per_device_eval_batch_size=4,
        gradient_accumulation_steps=4,         # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ batch_size=16
        learning_rate=3e-5,                    # –°–ª–µ–≥–∫–∞ —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π lr
        warmup_ratio=0.1,                      # –ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ —à–∞–≥–æ–≤
        weight_decay=0.01,
        logging_dir="./logs",
        logging_strategy="steps",
        logging_steps=50,
        save_strategy="epoch",                 # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É
        eval_strategy="epoch",                 # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        save_total_limit=3,                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º 3 –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏
        report_to="none",
        fp16=torch.cuda.is_available(),
        dataloader_num_workers=2,              # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        dataloader_pin_memory=True,
        remove_unused_columns=True,
        disable_tqdm=False,
        seed=42,
        prediction_loss_only=True,             # –¢–æ–ª—å–∫–æ loss –¥–ª—è LM
    )

    # Trainer —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        data_collator=data_collator,
        tokenizer=tokenizer,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Ç–µ—Ä–ø–µ–Ω–∏–µ
    )

    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–æ –æ–±—É—á–µ–Ω–∏—è
    print("\nüìà –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê...")
    try:
        eval_results = trainer.evaluate()
        print(f"üìä –ù–∞—á–∞–ª—å–Ω—ã–π loss: {eval_results['eval_loss']:.4f}")
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É: {e}")

    # –û–±—É—á–µ–Ω–∏–µ
    print("\nüî• –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï...")
    try:
        train_result = trainer.train()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        trainer.save_model()
        tokenizer.save_pretrained(OUTPUT_DIR)
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        metrics = train_result.metrics
        print(f"üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print(f"üìä Final train loss: {metrics.get('train_loss', 'N/A')}")
        print(f"üìä Final eval loss: {metrics.get('eval_loss', 'N/A')}")
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{OUTPUT_DIR}'")
        
        # –í—ã–≤–æ–¥ –ø—Ä–∏–º–µ—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        print("\nüß™ –¢–ï–°–¢ –ì–ï–ù–ï–†–ê–¶–ò–ò...")
        test_question = "–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç –¥–∏–ø–ª–æ–º–∞?"
        inputs = tokenizer(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {test_question}\n–°–∏—Å—Ç–µ–º–∞:", return_tensors="pt").to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_length=150,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"–í–æ–ø—Ä–æ—Å: {test_question}")
        print(f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {generated_text}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()